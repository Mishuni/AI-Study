# Regularization
> 모델이 data의 noise 까지 세세하게 학습하는 overfitting 이 일어나지 않도록 제한을 걸어주는 것 <br>
> image로 예를 들면, 사람은 이미지의 특정 패턴을 보고 인식하지만 컴퓨터는 모든 픽셀을 동일하게 보고 인식한다. 따라서 컴퓨터는 이미지 판단을 더욱 쉽게 만들거나 간단화할 수 있는 도움이 필요하다. ex) feature extractor, regularization, attention

- 여러 방법들
    - Agumented error 를 사용하는 방법 <br>
    <img src="..\images\argument_error.jpeg" alt="alexnet" style="zoom:10%;" />

    - noise 를 input/hidden units(=dropout)/weights/output target 등에 넣는 방법

    - mixup, multi-task learning, parameter sharing, early stopping, ensembling

    - regularizer 선택 : heuristic (L1 lasso, L2 ridge=`weight decay`)


## overfitting

> 모델이 data 에 대해 정도가 지나치게 fitting 하는 것 <br>
> 즉, 최적화 과정에서 train error 만으로는 더 이상 부족하다는 것

- 특히 잘 발견되는 경우
    - 모델이 target 을 나타내는데 필요한 것 보다 이상으로 complex한 경우
    - 모델이 추가적인 DOF(degree of freedom)을 데이터의 노이즈에 맞추는데 사용하는 경우

### bias vs variance

- 우리 모델이 너무 간단하고 적은 parameter 갯수를 가지고 있으면 높은 bias 와 작은 variance를 갖는다. (underfitting) 반대로, 모델이 너무 많은 수의 parameter를 가지고 있다면 이 모델은 높은 variance 와 낮은 bias 를 갖는다. (overfitting)

- VC generalization bound
<img src="..\images\vc_generalization_bound.jpeg" alt="alexnet" style="zoom:15%;" />

---

## regularization strategies

1. 모델에 조건을 걸어주기
    - ex) parameter 의 최대 변화량을 지정

2. 목적 함수(objective function)에 추가적인 식을 넣기
    -  parameter 값에 대한 soft 한 조건이라고 할 수 있음

3. training data 를 설명할 다량의 hypotheses 를 결합하기
    - ensemble methods

---

## Regularizers

> 보통 bias 에는 regularization 을 하진 않음

1. L1 regularizer (Lasso)
    - convex 하지만 어디서나 미분가능하진 않음
    - weight 의 값이 사라지고 + 선택되는 => sparse soulution

2. L2 regularizer (Ridge)
    - convex 하고 어디서나 미분 가능함
    - weight 의 값이 사라지는 것만 함
    - weight decay 로 불림 : weight 이 parameter update 할수록 작아지기 떄문에

> The obvious disadvantage of ridge regression, is model interpretability. It will shrink the coefficients for least important predictors, very close to zero. But it will never make them exactly zero. In other words, the final model will include all predictors. However, in the case of the lasso, the L1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. Therefore, the lasso method also performs variable selection and is said to yield sparse models. 